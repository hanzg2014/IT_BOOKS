{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "This recipe shows how we will be accessing the datasets necessary for the rest of the book.\n",
    "\n",
    "We start by loading the necessary libraries and resetting the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris Dataset (R. Fisher / Scikit-Learn)\n",
    "\n",
    "One of the most frequently used ML datasets is the iris flower dataset.  We will use the easy import tool, `datasets` from scikit-learn.  You can read more about it here: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "[ 5.1  3.5  1.4  0.2]\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(len(iris.data))\n",
    "print(len(iris.target))\n",
    "print(iris.data[0])\n",
    "print(set(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Birthrate Dataset (UMass/Amherst)\n",
    "\n",
    "The 'Low Birthrate Dataset' is a dataset provided by Univ. of Massachusetts at Amherst.  It is a great dataset used for numerical prediction (birthweight) and logistic regression (binary classification, birthweight`<`2500g or not). Information about it is located here:\n",
    "https://www.umass.edu/statdata/statdata/data/lowbwt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "birthdata_url = 'https://www.umass.edu/statdata/statdata/data/lowbwt.dat'\n",
    "birth_file = requests.get(birthdata_url)\n",
    "birth_data = birth_file.text.split('\\r\\n')[5:]\n",
    "birth_header = [x for x in birth_data[0].split(' ') if len(x)>=1]\n",
    "birth_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in birth_data[1:] if len(y)>=1]\n",
    "print(len(birth_data))\n",
    "print(len(birth_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Price Dataset (UCI)\n",
    "\n",
    "We will also use a housing price dataset from the University of California at Irvine (UCI) Machine Learning Database Repository. It is a great regression dataset to use.  You can read more about it here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "housing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "housing_header = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "housing_file = requests.get(housing_url)\n",
    "housing_data = [[float(x) for x in y.split(' ') if len(x)>=1] for y in housing_file.text.split('\\n') if len(y)>=1]\n",
    "print(len(housing_data))\n",
    "print(len(housing_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Handwriting Dataset (Yann LeCun)\n",
    "\n",
    "The MNIST Handwritten digit picture dataset is the `Hello World` of image recognition. The famous scientist and researcher, Yann LeCun, hosts it on his webpage here, http://yann.lecun.com/exdb/mnist/ .  But because it is so commonly used, many libraries, including TensorFlow, host it internally.  We will use TensorFlow to access this data as follows.\n",
    "\n",
    "If you haven't downloaded this before, please wait a bit while it downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "10000\n",
      "5000\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(mnist.train.labels[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham/Spam Texts Dataset (UCI)\n",
    "\n",
    "We will use another UCI ML Repository dataset called the SMS Spam Collection.  You can read about it here: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection .  As a sidenote about common terms, when predicting if a data point represents 'spam' (or unwanted advertisement), the alternative is called 'ham' (or useful information).\n",
    "\n",
    "This is a great dataset for predicting a binary outcome (spam/ham) from a textual input.  This will be very useful for short text sequences for Natural Language Processing (Ch 7) and Recurrent Neural Networks (Ch 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "{'spam', 'ham'}\n",
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Get/read zip file\n",
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(zip_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('SMSSpamCollection')\n",
    "# Format Data\n",
    "text_data = file.decode()\n",
    "text_data = text_data.encode('ascii',errors='ignore')\n",
    "text_data = text_data.decode().split('\\n')\n",
    "text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "print(len(text_data_train))\n",
    "print(set(text_data_target))\n",
    "print(text_data_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Data (Cornell)\n",
    "\n",
    "The Movie Review database, collected by Bo Pang and Lillian Lee (researchers at Cornell), serves as a great dataset to use for predicting a numerical number from textual inputs.\n",
    "\n",
    "You can read more about the dataset and papers using it here:\n",
    "https://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "simplistic , silly and tedious . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "r = requests.get(movie_data_url)\n",
    "# Stream data into temp object\n",
    "stream_data = io.BytesIO(r.content)\n",
    "tmp = io.BytesIO()\n",
    "while True:\n",
    "    s = stream_data.read(16384)\n",
    "    if not s:  \n",
    "        break\n",
    "    tmp.write(s)\n",
    "stream_data.close()\n",
    "tmp.seek(0)\n",
    "# Extract tar file\n",
    "tar_file = tarfile.open(fileobj=tmp, mode=\"r:gz\")\n",
    "pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "# Save pos/neg reviews\n",
    "pos_data = []\n",
    "for line in pos:\n",
    "    pos_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "neg_data = []\n",
    "for line in neg:\n",
    "    neg_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "tar_file.close()\n",
    "\n",
    "print(len(pos_data))\n",
    "print(len(neg_data))\n",
    "print(neg_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Works of William Shakespeare (Gutenberg Project)\n",
    "\n",
    "For training a TensorFlow Model to create text, we will train it on the complete works of William Shakespeare.  This can be accessed through the good work of the Gutenberg Project.  The Gutenberg Project frees many non-copyright books by making them accessible for free from the hard work of volunteers.\n",
    "\n",
    "You can read more about the Shakespeare works here:\n",
    "http://www.gutenberg.org/ebooks/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5582212\n"
     ]
    }
   ],
   "source": [
    "# The Works of Shakespeare Data\n",
    "import requests\n",
    "\n",
    "shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "# Get Shakespeare text\n",
    "response = requests.get(shakespeare_url)\n",
    "shakespeare_file = response.content\n",
    "# Decode binary into string\n",
    "shakespeare_text = shakespeare_file.decode('utf-8')\n",
    "# Drop first few descriptive paragraphs.\n",
    "shakespeare_text = shakespeare_text[7675:]\n",
    "print(len(shakespeare_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English-German Sentence Translation Database (Manythings/Tatoeba)\n",
    "\n",
    "The Tatoeba Project is also run by volunteers and is set to make the most bilingual sentence translations available between many different languages.  `Manythings.org` compiles the data and makes it accessible.\n",
    "\n",
    "http://www.manythings.org/corpus/about.html#info\n",
    "\n",
    "More bilingual sentence pairs: http://www.manythings.org/bilingual/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147788\n",
      "147788\n",
      "['I won!', 'Ich hab gewonnen!']\n"
     ]
    }
   ],
   "source": [
    "# English-German Sentence Translation Data\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "r = requests.get(sentence_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('deu.txt')\n",
    "# Format Data\n",
    "eng_ger_data = file.decode()\n",
    "eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x)>=1]\n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\n",
    "print(len(english_sentence))\n",
    "print(len(german_sentence))\n",
    "print(eng_ger_data[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
